% ADMM methods and Lemma for Latent 

A convex relaxation approach replaces the constraint $\text{rank}(\W_{(n)})$ with its convex hull $\|\W_{(n)}\|_*$.  The mixture regularization in \cite{tomioka2010estimation} assumes that the $N$-mode tensor $\W$ is a mixture of $N$ auxiliary tensors $\{\Z^{n}\}$, i.e., $\W = \sum_{n=1}^{N}\Z^n$. It regularizes the nuclear norm of only the mode-$n$ unfolding for the $n$ th tensor $\Z^{n}$, i.e, $\sum_{n=1}^N \|\Z^n_{(n)}\|_*$. The resulting convex relaxed optimization problem is as follows:

\vspace{-0.3in}
\begin{align}\label{eqn:mixture}
\widehat{\W} = \argmin_{\W}\left\{\loss(\W;\Y, \V )   + \lambda \sum_n^N \|\Z^n_{(n)}\|_*  \quad
\mathrm{s.t.} \quad  \sum\limits_n^N \Z^n = \W\right\}
\end{align} 
\vspace{-0.1in}


We adapt Alternating Direction Methods of Multiplier  (ADMM) \cite{gabay1976dual} for solving the above problem. Due to the coupling of $\{ \Z^n \}$ in the summation, each $\Z^n$ is not directly separable from other $\Z^{n'}$. Thus, we employ the coordinate descent algorithm to sequentially solve $\{\Z^n\}$. Given the augmented Lagrangian of problem as follows, the ADMM-based algorithm is elaborated in Algo. \ref{alg:ADMM}. 

\begin{eqnarray}\label{eqn:AL}
F(\mathcal{\W},\{\mathcal{Z}^n\},\mathcal{C}) =  \loss(\W;\Y, \V )  + \lambda \sum\limits_{n=1}^N\|\mathcal{Z}^n_{(n)}\|_*  +  \frac{\beta}{2}\sum\limits_n \|\mathcal{\W}-\sum\limits_n\mathcal{Z}^n \|_F^2   -  \langle\mathcal{C}, \mathcal{\W}-\sum\limits_{n=1}^N\mathcal{Z}^n\rangle
\end{eqnarray}


%\begin{algorithm}
%   \caption{ADMM for cokriging with mixture regularizer}
%   \label{alg:ADMM}
%\begin{algorithmic}[1]
%   \STATE {\bfseries Input:} data $\X$ with Laplacian matrix $L$, hyper-parameters $\lambda,\beta$.
%   \STATE {\bfseries Output:} $N$ mode tensor $\W$ 
%   \STATE Initialize $\W, \{\Z^{n}\}, \C$.
%   \REPEAT
%   \FOR{variable $m=1$ {\bfseries to} $M$}
%   \STATE $\W_{:,:,m} \leftarrow   (2 \lambda L  + (\beta + N) I)^{-1}(\frac{1}{\lambda}\X_{:,:,m} + \mathcal{C} + \frac{\beta}{N}\sum_{n=1}^N \mathcal{Z}^n) $
%   \ENDFOR
%   \REPEAT
%   \STATE $\Sigma =  N\W_{(n)}-\frac{N}{\beta}\C - \sum_{n'\neq n} \Z^{n'}_{(n')}$
%   \STATE $\Z^n_{(n)} = \mathrm{shrink}\left( \Sigma , \frac{N^2\lambda}{\beta} \right)$
%   \UNTIL{ solution $\{\Z^n\}$ converge}
%   \STATE $\mathcal{C} \leftarrow  \mathcal{C}-\beta(\mathcal{\W}-\frac{1}{N} \sum\limits_{n=1}^N Z^{n})$
%   
%   \UNTIL{objective function converges}
%\end{algorithmic}
%\end{algorithm}

\begin{algorithm}[h]
 \caption{ADMM for solving Eq. (\ref{eq:greedyUnified})}
 \label{alg:ADMM}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:}
   transformed data $\Y, \V$ of $M$ variables, hyper-parameters $\lambda,\beta$.
   \STATE {\bfseries Output:} $N$ mode tensor $\W$ 
   \STATE Initialize $\W, \{\Z^{n}\}, \C$ to zero.
   \REPEAT
   \STATE $\W \leftarrow   \argmin_{\W}\left\{\loss(\W;\Y, \V ) +\frac{\beta}{2}\|\W - \sum_{n=1}^{N}\Z^{n} - \C  \|_F^2 \right\}$.
   \REPEAT
   \FOR{variable $n=1$ {\bfseries to} $N$}
   \STATE $\Z^n_{(n)} = \mathrm{shrink}_{\frac{\lambda}{\beta}}\left( \W_{(n)}-\frac{1}{\beta}\C - \sum_{n'\neq n} \Z^{n'}_{(n')}  \right)$.
   \ENDFOR
   \UNTIL{ solution $\{\Z^n\}$ converge}
   \STATE $\mathcal{C} \leftarrow  \mathcal{C}-\beta(\mathcal{\W}- \sum\limits_{n=1}^N \Z^{n})$.
   
   \UNTIL{objective function converges}
\end{algorithmic}
\end{algorithm}


%For forecasting, we replace steps 5-6 in Algo. \ref{alg:ADMM} with a gradient descend step for solving $\W_{:,:,m}$ as the resulting close form solution is in the form of Sylverster equation and is expensive to solve. 
The sub-routine $\mathrm{shrink}_{\alpha} (A) $ applies a soft-thresholding rule at level $\alpha$ to the singular values of the input matrix  $A$. The following lemma shows the convergence of ADMM-based solver for our problem.

\begin{lemma} \cite{bertsekas1989parallel}
\label{lem:admm_opt}
For the constrained problem $\min\limits_{x,y} f(x)+g(y), \mathrm{s.t} \quad x \in C_x, y\in C_y, Gx=y$, If either $\{C_x, C_y\}$ are bounded or $G'G$ is invertible, and the optimal solution set is nonempty. A sequence of solutions $\{x, y\}$ generated by ADMM is bounded and every limit point is an optimal solution of the original problem.
\end{lemma}
%\begin{theorem}
%If the optimal solution set of problem (\ref{eqn:mixture})  is nonempty, algorithm \ref{alg:ADMM} generates a bounded sequence of $\{\W,\Z^n, \mathcal{C}\}$ and every limit point of $\W$ is an optimal solution. 
%\end{theorem}
%\proof It easily follows that problem (\ref{eqn:mixture}) satisfied invertible condition for $\W$. With Lemma \ref{lem:admm_opt}, we can reach the conclusion.


