We conduct cokriging and forecasting experiments on three real-world datasets: 
\vspace{-3mm}
\paragraph{USHCN}
The U.S. Historical  Climatology Network Monthly (USHCN) \footnote{\url{ http://www.ncdc.noaa.gov/oa/climate/research/ushcn}} dataset consists of monthly climatological data of 108 stations spanning from year 1915 to 2000. It has three climate variables: (1) daily maximum, (2) minimum temperature  averaged over month, and (3) total monthly precipitation. 
\vspace{-3mm}
\paragraph{CCDS}
The Comprehensive Climate Dataset (CCDS) is a collection of climate records of North America from \cite{lozano2009spatial}. The dataset was collected and pre-processed by five federal agencies. %:  CRU (\url{http://www.cru.uea.ac.uk/cru/data }), NOAA (\url{ http://www.cdc.noaa.gov/data/gridded/}), NASA (\url { http://iridl.ldeo.columbia.edu/ SOURCES/.NASA/ .GSFC/.TOMS/.NIMBUS7/ }), NCDC (\url{http://rredc.nrel.gov/solar/old_data/nsrdb/ }) and CDIAI (\url{http://cdiac.ornl.gov/epubs/ndp/ushcn/ usa.html)}) 
It contains monthly observations of 17 variables such as Carbon dioxide and temperature spanning from 1990 to 2001. The observations were interpolated on a $2.5 \times 2.5$ degree grid, with 125 observation locations.
\vspace{-3mm}
\paragraph{Foursquare}
The Foursquare dataset \cite{long2012exploring} contains the users' check-in records in Pittsburgh area from Feb 24 to May 23, 2012, categorized by different venue types such as Art \& Entertainment, College \& University, and Food. The check-in tensor is created by counting the number of check-ins by 121 users  in each of the 15 category of venues during 1200 time intervals.
%We select a subset of dataset with 121 active users for 3767 time stamps of 15 venue category variables. 

\begin{table*}[t]
\caption{Cokriging NRMSE of 6 methods averaged over 10 runs. In each run, 10\% of the locations are assumed missing. } %title of the table
\label{tab:cokrig_RMSE}
%\vskip 0.15in
\begin{center}
\begin{tiny}
\begin{sc}
\centering  \footnotesize% centering table
\begin{tabular}{c c c c c c c c} % creating eight columns
%\abovespace\belowspace
\hline
Dataset & ADMM & Forward & Orthogonal  & Simple& Ordinary& MTGP \\
\hline
USHCN  &  0.8051 & 0.7594 & \textbf{0.7210}&  0.8760& 0.7803 & 1.0007 \\
CCDS & 0.8292 & 0.5555& \textbf{0.4532} & 0.7634 & 0.7312 & 1.0296 \\
Foursquare & 0.1373 & 0.1338& \textbf{0.1334} & NA & NA & NA \\
\hline
\end{tabular}
\end{sc}
\end{tiny}
\end{center}
\vspace{-0.25in}
\end{table*}

% cokring
\subsubsection{Cokriging}
We compare the cokriging performance of our proposed method with the classical cokriging approaches including simple kriging and ordinary cokriging with nonbias condition \cite{isaaks2011applied} which are applied to each variables separately. We further compare with multitask Gaussian process (MTGP) \cite{bonilla2007multi} which also considers the correlation among variables. We also adapt ADMM for solving the nuclear norm relaxed formulation of the cokriging formulation as a baseline (see  Appendix \ref{sec:admm_algo} for more details). For USHCN and CCDS, we construct a Laplacian matrix by calculating the pairwise Haversine distance of locations. For Foursquare, we construct the graph Laplacian from the user friendship network.

For each dataset, we first normalize it by removing the trend and diving by the standard deviation. Then we randomly pick 10\% of locations (or users for Foursquare) and eliminate the measurements of all variables over the whole time span. Then, we produce the estimates for all variables of each timestamp. We repeat the procedure for 10 times and report the normalized prediction  RMSE for all timestamps and 10 random sets of missing locations, i.e., NRMSE $=\|\widehat{\X}_{\Omega^c} - \X_{\Omega^c}\|_F/\|\X_{\Omega^c}\|_F$, where $\Omega^c$ denotes the unobserved locations. We use the  MATLAB Kriging Toolbox\footnote{\url{ http://globec.whoi.edu/software/kriging/V3/english.html}} for the classical cokriging algorithms and the MTGP code provided by \cite{bonilla2007multi}. 

Table \ref{tab:cokrig_RMSE} shows the  results for the cokriging task. The greedy algorithm with orthogonal projections is significantly more accurate in all three datasets. The baseline cokriging methods can only handle the two dimensional longitude and latitude information, thus are not applicable to the Foursquare dataset with additional friendship information. The superior performance of the greedy algorithm can be attributed to two of its properties: (1) It can obtain low rank models and achieve global consistency;  (2) It usually has lower estimation bias compared to nuclear norm relaxed methods. %During our experiments, we observe that the Laplacian  regularization with geographical information further improves the performance with the local consistency constraint.



% 0.6138 CCDS For
