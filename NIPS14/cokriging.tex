% cokriging description
In geostatistics, cokriging is the task of interpolating the data of one variable for unknown locations by taking advantage of the observations of variables from known locations. For example, by making use of the correlations between precipitation and temperature, we can obtain more precise estimate of temperature in unknown locations than univariate kriging. Formally, denote the complete data for $P$ locations over $T$ time stamps with $M$ variables as $\X \in \mathbb{R}^{P\times T\times M} $. We only observe the measurements for a subset of locations $\Omega \subset \{1, \ldots, P\}$ and their side information such as longitude and latitude. Given the measurements $\X_{\Omega}$ and the side information, the goal is to estimate a tensor $\W \in \mathbb{R}^{P\times T\times M}$ that satisfies $\W_\Omega =  \X_\Omega$. Here $\X_\Omega$ represents the outcome of applying the index operator $I_\Omega$ to $\X_{:, :, m}$ for all variables $ m= 1, \ldots, M$. The index operator $I_\Omega$ is a diagonal matrix whose entries are one for the locations included in $\Omega$ and zero otherwise.


Existing work have identified two key consistency principles for performing cokriging \cite[Chapter 6.2]{cressie2011statistics}: (1) Global consistency: the data in the common structure (variable, space, time) are likely to be similar. (2) Local consistency: the data in close locations are likely to be similar. These principles are akin to the \textit{cluster assumption} for semi-supervised learning \cite{zhou2003learning}. We incorporate these principles in a concise and computationally efficient low-rank tensor learning framework.

To achieve global consistency, we constrain the tensor $\W$ to be low rank. The low rank assumption is based on the belief that high correlations exist within variables, locations and time, which leads to natural clustering of the data. Existing literature have explored the low rank structure among these three dimensions separately, e.g., multi-task learning \cite{nie2010efficient} for variable correlation, fixed rank kriging \cite{cressie2008fixed} for spatial correlations. Low rankness assumes that the observed data can be described with a few latent factors. It enforces the commonalities along three dimensions without an explicit form for the shared structures in each dimension.

For the local consistency, we construct a regularizer via the spatial Laplacian matrix. The Laplacian matrix is defined as  $L = D-A$, where $A$ is a kernel matrix constructed by pairwise similarity %$A_{i,j} = \mathrm{similarity}(\mathrm{location}(i), \mathrm{location}(j))$
and diagonal matrix $D_{i,i}= \sum_{j} (A_{i,j})$. %For every vector $\mathbf{x}$, the Laplacian regularizer is applied as $\mathrm{tr}(\mathbf{x}^{\top}L\mathbf{x}) = \sum_{i, j}A_{i,j}(x_i-x_j)^2$ which smoothes the entries of $\mathbf{x}$ that are similar according to $A$; 
Similar ideas have been used in matrix completion \cite{li2009relation}. In cokriging literature, the local consistency is enforced via the spatial covariance matrix. The Bayesian models often impose the Gaussian process prior on the observations with the covariance matrix $K = K_v \otimes K_x $ where $K_v$ is the covariance between variables and $K_x$ is that for locations. The Laplacian regularization term corresponds to the relational Gaussian process \cite{chu2006relational} where the covariance matrix is approximated by the spatial Laplacian.


In summary, we can perform cokriging and find the value of tensor $\W$ by solving the following optimization problem:
\begin{align}
\widehat{\W} =\argmin_{\W}& \left\{ \|\W_\Omega - \X_\Omega \|^2_F +  \mu \sum\limits_{m=1}^M \text{tr} (\W_{:,:,m}^\top L \W_{:,:,m}) \right\} \quad
\mathrm{s.t.} \;&  \text{rank}(\W) \leq \rho,  \label{eqn:cokriging}
\end{align}
\noindent  where the Frobenius norm of a tensor $\A$ is defined as $\|\A\|_F = \sqrt{\sum_{i, j, k}\A_{i, j, k}^2}$ and $\mu, \rho > 0$ are the parameters that make  tradeoff between the local and global consistency, respectively.  The low rank constraint finds the principal components of the tensor and reduces the complexity of the model while the Laplacian regularizer clusters the data using the relational information among the locations.  By learning the right tradeoff between these two techniques, our method is able to benefit from both of them. Due to the various definitions of tensor rank, we use \textit{rank} as supposition for rank complexity, which will be specified in later section.
