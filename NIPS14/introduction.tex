%introduction
Spatio-temporal data provide unique information regarding ``where'' and ``when'', which is essential to answer many important  questions in scientific studies from geology, climatology to sociology. From a machine learning perspective, we are confronted with a series of new challenges when analyzing spatio-temporal data because of the complex spatial and temporal dependencies involved. 

A plethora of excellent work has been conducted to address the challenge and achieved successes to a certain extent, see e.g. \cite{cressie2010fixed, isaaks2011applied} and the references therein. Often times, geostatistical models use cross variogram and cross covariance functions to describe the intrinsic dependency structure. However, the parametric form of the cross variogram and the cross covariance functions impose strong assumptions on the correlation among the data. Moreover, choosing a valid variogram function requires domain knowledge and manual work. Most importantly, parameter learning of those statistical models is computationally expensive, making them infeasible for  large-scale spatio-temporal applications. 

Cokriging and forecasting are the two central tasks in multivariate spatio-temporal analysis. Cokriging utilizes the spatial correlations to predict the value of the variables for new locations. One widely adopted method is the  Multitask Gaussian Process (MTGP) \cite{bonilla2007multi}, which assumes the Gaussian process prior on the observations. However, for a cokriging task with $M$ variables of $P$ locations for $T$ time stamps, the time complexity of MTGP is $\mathcal{O}(M^3P^3T)$. 
%Acceleration techniques such as Nystr{\"o}m method and incomplete-Cholesky decomposition are employed, but they do not fully capture the correlations. 
For forecasting,  popular methods in multivariate time series analysis include the vector autoregressive (VAR) model, the autoregressive integrated moving average (ARIMA) model, and cointegration models. Bayesian hierarchical spatio-temporal models have studied both space-time separable and non-separable covariance functions \cite{cressie1999classes}. Rank reduced models have been proposed to incorporate the interrelationship among variables \cite{anderson1951estimating}. However, none of the existing work handles the commonalities among variables, space and time simultaneously in a scalable way. In this paper, we aim to address this problem by presenting a unified framework for many spatio-temporal analysis tasks that are scalable for large-scale applications. % without assuming the explicit form of the commonalities among those dimensions. 

%Since the multivariate spatio-temporal data come in the form of (variable $\times$ time $\times$ location), it is natural to represent it using tensors. 
Tensor representation provides a convenient way to incorporate the inter-dependencies along multiple dimensions. Therefore it is natural to represent the multivariate spatio-temporal data  in tensor.  Recent advances in low rank learning have led to simple models that can  capture the commonalities among each mode of the tensor and produce simpler models, which are easier to learn. Similar assumptions can be seen in the literature of spatial data recovery \cite{gandy2011tensor}, neuroimaging analysis \cite{zhou2013tensor}, and  multi-task learning \cite{romera2013multilinear}. Our work builds upon the recent advances in low rank tensor learning \cite{kolda2009tensor, gandy2011tensor, zhou2013tensor} and further considers the scenario where additional side information for the data is available. For the geo-spatial applications, apart from measurements of multiple variables, we also utilize the geographical information. For social network applications, we take advantage of the friendship network structure.  To utilize the side information, we construct similarity kernel based on them and regularize with the corresponding Laplacian matrix, which favors locally smooth solutions.

We develop a fast greedy algorithm for learning low rank tensors based on the greedy structure learning framework \cite{Barron2008,Zhang2011,Shwartz11}.  
The greedy low rank tensor learning is efficient, as it does not require full singular value decomposition of large matrices as opposed to other alternating direction methods. 
We also provide a bound on the difference between the loss function at the greedy algorithm solution and  the globally optimal solution. %\ryedit{Singular value statement is too technical for introduction. What is the condition for global optimality.}
 Finally, we present simulation results as well as the empirical evaluation results on climate and social network data where our algorithm achieves higher accuracy with increased speed than state-of-art approaches in cokriging and forecasting tasks.

%\ryedit{We don't need separate notation paragraph, introduce when first appear}
%\paragraph{Notations} In this paper, we use lowercase letter for scalers, bold font small letter for vectors, capital letter for matrices and calligraphic font for tensors. We describe the tensor basics and notations to be used in this paper. 
% We use three types of matrix norms: For any matrix $A \in \mathbb{R}^{p\times q}$ with singular values $\sigma_1 \geq \sigma_2 \geq \ldots \sigma_{\min(p, q)}\geq 0$, the \textit{Frobenius norm} is defined as $\|A\|_F = \sqrt{\sum_{i=1}^{\min(p, q)} \sigma_i^2 }$ and the \textit{Nuclear norm} (also called \textit{Trace norm}) is defined as $\|A\|_* = \sum_{i=1}^{\min(p, q)} \sigma_i$ and the \textit{Operator norm} is defined as $\|A\|_{op} = \|A\|_2 = \sigma_1$. We also use the colon operator : to select a slice of a tensor similar to MATLAB's syntax.
