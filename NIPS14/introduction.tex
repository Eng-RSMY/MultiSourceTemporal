%introduction
Spatio-temporal data provide unique information regarding ``where'' and ``when'', which is essential to answer many important  questions in scientific studies from geology, climatology to sociology. In the context of big data, we are confronted with a series of new challenges when analyzing spatio-temporal data because of the complex spatial and temporal dependencies involved. 

A plethora of excellent work has been conducted to address the challenge and achieved successes to a certain extent  \cite{cressie2010fixed, isaaks2011applied}. Often times, geostatistical models use cross variogram and cross covariance functions to describe the intrinsic dependency structure. However, the parametric form of cross variogram and cross covariance functions impose strong assumptions on the spatial and temporal correlation, which requires domain knowledge and manual work. Furthermore, parameter learning of those statistical models is computationally expensive, making them infeasible for  large-scale applications. 

Cokriging and forecasting are two central tasks in multivariate spatio-temporal analysis. Cokriging utilizes the spatial correlations to predict the value of the variables for new locations. One widely adopted method is  multitask Gaussian process (MTGP) \cite{bonilla2007multi}, which assumes a Gaussian process prior 
over latent functions to directly induce correlations between tasks. However, for a cokriging task with $M$ variables of $P$ locations for $T$ time stamps, the time complexity of MTGP is $\mathcal{O}(M^3P^3T)$ \cite{}. 
%Acceleration techniques such as Nystr{\"o}m method and incomplete-Cholesky decomposition are employed, but they do not fully capture the correlations. 
For forecasting,  popular methods in multivariate time series analysis include  vector autoregressive (VAR) models, autoregressive integrated moving average (ARIMA) models, and cointegration models. An alternative method for spatio-temporal analysis  is Bayesian hierarchical spatio-temporal models with either  separable and non-separable space-time covariance functions \cite{cressie1999classes}. Rank reduced models have been proposed to capture the inter-dependency among variables \cite{anderson1951estimating}. However, very few models can directly handle the correlations among variables, space and time simultaneously in a scalable way. In this paper, we aim to address this problem by presenting a unified framework for many spatio-temporal analysis tasks that are scalable for large-scale applications. % without assuming the explicit form of the commonalities among those dimensions. 

%Since the multivariate spatio-temporal data come in the form of (variable $\times$ time $\times$ location), it is natural to represent it using tensors. 
Tensor representation provides a convenient way to capture inter-dependencies along multiple dimensions. Therefore it is natural to represent the multivariate spatio-temporal data  in tensor.  Recent advances in low rank learning have led to simple models that can  capture the commonalities among each mode of the tensor \cite{}. Similar argument can be found in the literature of spatial data recovery \cite{gandy2011tensor}, neuroimaging analysis \cite{zhou2013tensor}, and  multi-task learning \cite{romera2013multilinear}. Our work builds upon  recent advances in low rank tensor learning \cite{kolda2009tensor, gandy2011tensor, zhou2013tensor} and further considers the scenario where additional side information of data is available. For example, in geo-spatial applications, apart from measurements of multiple variables,  geographical information is available to infer location adjacency; in social network applications,  friendship network structure is collected to obtain preference similarity.  To utilize the side information, we can construct a Laplacian regularizer from the similarity matrices, which favors locally smooth solutions.

We develop a fast greedy algorithm for learning low rank tensors based on the greedy structure learning framework \cite{Barron2008,Zhang2011,Shwartz11}.  
Greedy low rank tensor learning is efficient, as it does not require full singular value decomposition of large matrices as opposed to other alternating direction methods \cite{refs for alternating direction methods}. 
We also provide a bound on the difference between the loss function at our greedy solution and the one at the globally optimal solution. %\ryedit{Singular value statement is too technical for introduction. What is the condition for global optimality.}
 Finally, we present experiment results on simulation datasets as well as application datasets in  climate and social network analysis, which shows that our algorithm is faster and achieves higher prediction accuracy than state-of-art approaches in cokriging and forecasting tasks.

%\ryedit{We don't need separate notation paragraph, introduce when first appear}
%\paragraph{Notations} In this paper, we use lowercase letter for scalers, bold font small letter for vectors, capital letter for matrices and calligraphic font for tensors. We describe the tensor basics and notations to be used in this paper. 
% We use three types of matrix norms: For any matrix $A \in \mathbb{R}^{p\times q}$ with singular values $\sigma_1 \geq \sigma_2 \geq \ldots \sigma_{\min(p, q)}\geq 0$, the \textit{Frobenius norm} is defined as $\|A\|_F = \sqrt{\sum_{i=1}^{\min(p, q)} \sigma_i^2 }$ and the \textit{Nuclear norm} (also called \textit{Trace norm}) is defined as $\|A\|_* = \sum_{i=1}^{\min(p, q)} \sigma_i$ and the \textit{Operator norm} is defined as $\|A\|_{op} = \|A\|_2 = \sigma_1$. We also use the colon operator : to select a slice of a tensor similar to MATLAB's syntax.
